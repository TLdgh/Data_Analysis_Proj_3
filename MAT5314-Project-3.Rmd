---
title: |
  | \vspace{-4em}MAT5314 Project 3: Queueing Theory
author: 
  - Teng Li(7373086)
  - Shiya Gao(300381032) 
  - Chuhan Yue(300376046)
  - Yang Lyu(8701121)
output: 
  pdf_document: 
    keep_tex: true
    includes:
      in_header: columns.tex
fontsize: 11pt
header-includes: 
  - \renewcommand{\and}{\\}
  - \usepackage{float}
  - \floatplacement{figure}{H}
bibliography: References.bib
link-citations: yes
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(plotly)
library(tidyverse)
library(kableExtra)
library(htmlwidgets)
library(webshot)
library(randomForest)
```

```{r, include=FALSE}
source("CodeSpace.R")
```

### Introduction

In this project we aimed to analyze two data sets with Bayesian techniques. One can see a different way of doing statistical inference and interpreting results, which offers some of the benefits that the usual frequentist approach wouldn't be able to do. We used the two data sets to illustrate the benefits, as well as explaining it from an intuitive perspective. Moreover, we aimed to target the audience who has some fundamental knowledge of Probability and Statistics so that the core features of Bayesian techniques can be understood.

### Bayesian Analysis in A/B Test

A webpage designer might be interested in the goodness of the new design compared with the old one. He wants to discover whether the new web page had a higher conversion rate than the old one. The designer made an experiment such that participants were randomly given a new webpage as the treatment or an old webpage as a control. Each participant decides if he/she wants to "convert", or in other words to make a decision. For simplicity, conversion can only be yes or no. In addition, each participant's decision was treated as independent. This formed a valid designed experiment to allow the designer to make a causal inference. The resulted data formed a random sample of Bernoulli trials. Suppose $X_1,...,X_n$ are the random sample of the participant's choice, with $X_i=1$ if a participant chooses to convert to the webpage assigned to him with probability $p$, otherwise $X_i=0$ with probability $1-p$. The success rate $p$ is the parameter of the Bernoulli distribution. Our goal was therefore to compare the conversion rate $p_1$ and $p_2$, where $p_1$ is the rate for the control group and $p_2$ is the rate for the treatment group.

From a business perspective, the designer wants the conversion rate to be as high as possible for the new webpage. One can have various hypothesis tests on the conversion rate $p$. One may consider an A/B test:

$$
H_0: ~ p_1 \geq p_2 \quad H_1: ~ p_1 < p_2
$$

such that one would like to know if the conversion rate for the new page is greater than the one for the old page. 

In a frequentist approach, one treats the parameters $p_1$, $p_2$ as unknown but fixed values and must find a test statistic and its distribution under the null hypothesis. This is in general difficult. With Bayesian analysis however, the parameters are treated as random variables, and one can evaluate directly the probability of the null and the alternative hypotheses. In our webpage experiment, this is more intuitive, because the conversion rate for the webpage should change constantly as more visitors visit the webpage. It does not necessarily need to converge to some fixed values either, for example a social trend may influence the preference of visitors when they "convert" to a feature. In addition, a valid statistical inference should usually provide a confidence interval. The interpretation of a confidence interval is important. In a frequentist point of view, our unknown parameter $p$ is true but fixed. The confidence interval on the other hand is a random set, because whenever the designer collects a new data sample, the confidence interval is calculated based on that particular sample. Thus it is incorrect to state that, for example, we are 99% sure that the confidence interval covers the unknown true parameter. Either the confidence interval covers the fixed value or not, with probability 1 or 0. It is also incorrect to say that the unknown parameter falls within the confidence interval 99% of the time, because in this statement the unknown parameter is falsely treated as a random variable. Therefore the appropriate interpretation of the confidence interval is: when the experiment is repeated many times to generate random samples and their perspective confidence intervals, 99% of these confidence intervals will cover the unknown fixed parameter. Once can immediately see that this statement is not easy to be introduced to the public.

If we use the Bayesian approach to gain a different understanding of our unknown parameter and to draw inference on it, we then get the ($1-\alpha$)-level credible interval for the conversion rate based on the quantile of the posterior distribution as $[L(\vec{x}), R(\vec{x})]$ such that:
$$
\mathbb{P}(\textbf{p} \in [L(\vec{x}), R(\vec{x})] |\vec{x}) = \int_{L(\vec{x})}^{R(\vec{x})} f(p|\vec{x})dp = 1-\alpha
$$

With such definition, it is perfectly fine to interpret the credible interval as: the probability that our success rate falls within the given credible interval is (1-$\alpha$)%. Hence, we see the benefit of using Bayesian interpretation.

To evaluate the probability of the hypothesis test, we used the Monte Carlo method to evaluate the following integral:

$$
P(p_1 < p_2 |\vec{x}) = \int_0^1 \int_0^1 \mathbb{I}(p_1 < p_2 |\vec{x}) \cdot f(p_1|\vec{x})f(p_2|\vec{x})dp_1 dp_2 = E[\mathbb{I}(p_1 < p_2 |\vec{x})]
$$

where $\mathbb{I}(p_1 < p_2|\vec{x})$ is the indicator function given the data and $f(p_1|\vec{x})$, $f(p_2|\vec{x})$ are the posterior probability density functions for control and treatment respectively. To estimate this probability, one only needs to simulate pairs of random variables from the two posterior distributions simultaneously, and calculate the average of the indicator function. 

As one can see, choosing the appropriate prior distribution to get the posterior is key in Bayesian inference. A prior distribution is a subjective "guess" of what the distribution of the target parameter should be. In our case we were trying to estimate the conversion rate $p$. We chose Beta distribution as the prior distribution because of three reasons:

1. We observed that Beta distribution has domain between 0 and 1, which was ideal for a conversion rate.

2. The Beta distribution can take on various shapes by changing its parameters. In other words, we can put more subjective opinion on where the conversion rate is most likely. 

3. Because of the fact that the random sample consisted of Bernoulli trials and the prior is the Beta distribution, the posterior distribution derived by the Bayes' Theorem is also a Beta distribution. Both of the prior and the posterior come from the same family of distributions, in which case we had a conjugate pair. The advantage was having an analytical form of the posterior from which we could easily sample. In general, the posterior distribution must be computationally approximated, which wouldn't be further discussed in our project.

Based on the explanation above, assuming p is the conversion rate with a prior $Beta(\alpha, \beta)$ distribution, then the posterior distribution is $Beta(\alpha+\sum_{i=1}^n x_i, \beta+n-\sum_{i=1}^n x_i)$ with the mean and variance:

\begin{gather*}
E(p|\vec{x})=\frac{\alpha+\sum_{i=1}^n x_i}{\alpha+\beta+n}\\
Var(p|\vec{x})=\frac{(\alpha+\sum_{i=1}^n x_i)(\beta+n-\sum_{i=1}^n x_i)}{(\alpha+\beta+n)^2(\alpha+\beta+n+1)}
\end{gather*}

One can see that asymptotically when we have a large sample size n, the mean of the posterior distribution approaches to the maximum likelihood estimator $\hat{p} =\frac{1}{n}\sum_{i=1}^n x_i$. Thus the specification of a prior is less important if the sample size is large. This allows us to make a subjective assumption of the success rate based on a probability distribution instead of viewing it as a fixed but unknown number in the frequentist hypothesis test. In the case where $\alpha=\beta=1$, we obtain the non-informative prior $Beta(1,1)$, which is $Uniform([0,1])$ distribution. It's called non-informative because we assume that all p are equally probable. In other words, we no longer put a subjective assumption on $p$.

Another advantage of doing Bayesian analysis is that the posterior distributions can be updated by continuously increasing the number of samples to form a new subset. The data from the previous experiment formed a posterior distribution, which can be used as the new "prior" for the next experiment. This allows a tester to update the prior degree of belief on the parameter by simply repeating the experiment, without having to recalculate everything from the beginning again.

\begin{minipage}[t]{0.55\textwidth}
```{r,fig.height=2, fig.width=6, fig.align='center', fig.cap = "Beta Distribution"}
parameters <- data.frame(
  alpha = c(0.5, 5, 1),
  beta = c(0.5, 1, 3)
)

x_values <- seq(0, 1, by = 0.01)

beta_data <- data.frame()

for (i in 1:nrow(parameters)) {
  alpha <- parameters$alpha[i]
  beta <- parameters$beta[i]
  density <- dbeta(x_values, shape1 = alpha, shape2 = beta)
  
  df <- data.frame(x = x_values, density = density, alpha = alpha, beta = beta)
  beta_data <- rbind(beta_data, df)
}

p<-ggplot(data = beta_data, aes(x = x, y = density, color = factor(alpha))) +
  geom_line() +
  labs(x = "X", y = "PDF") +
  scale_color_discrete(name = "parameters", labels=c("alpha=0.5, beta=0.5", "alpha=5, beta=1", "alpha=1, beta=3")) +
  theme_minimal()
print(p)
```
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
We must point out that, the choice of prior becomes prominent when the sample size is limited. Figure 1 showed the probability density function of the Beta distribution for three sets of values of the parameters $\alpha$ and $\beta$. From this one can see that different values of $\alpha$ and $\beta$ can have a great impact on the shape of the Beta distribution, the range of the distribution, and so on. 
\end{minipage}

Hence, when the sample size was small, the choice of prior becomes very important; appropriate and inappropriate choices could cause a huge impact on the resulted posterior distribution.

\begin{minipage}[t]{0.55\textwidth}
```{r, fig.height=2, fig.width=4, fig.align='center', fig.cap = "Posterior Density Comparison"}
ab_data<-read.csv("ab_data.csv", header = TRUE)

#cleanning the data
old <- ab_data %>% filter(landing_page == "old_page" & group == "control")
new <- ab_data %>% filter(landing_page == "new_page" & group == "treatment")
old_distinct <- old %>% distinct(old$user_id, .keep_all = TRUE)
new_distinct <- new %>% distinct(new$user_id, .keep_all = TRUE)

old_distinct <- old %>%head(100)

visitor_to_old <- nrow(old_distinct)
conversion_old <- sum(old_distinct$converted)

alpha_prior1 <- 2
beta_prior1 <- 20
alpha_prior2 <- 1
beta_prior2 <- 1000

# calculate posterior
posterior_old1 <- rbeta(1000000, alpha_prior1+conversion_old, beta_prior1+visitor_to_old-conversion_old)
posterior_old2 <- rbeta(1000000, alpha_prior2+conversion_old, beta_prior2+visitor_to_old-conversion_old)

posterior_Old1 <- data.frame(old_rate=c(posterior_old1))
posterior_Old2 <- data.frame(old_rate=c(posterior_old2))
mean_old1 <- mean(posterior_Old1$old_rate)
mean_old2 <- mean(posterior_Old2$old_rate)

ggplot() +
  geom_density(aes(x = posterior_Old1$old_rate), fill = "blue", alpha = 0.5) +
  geom_density(aes(x = posterior_Old2$old_rate), fill = "red", alpha = 0.5) +
  geom_vline(xintercept = mean_old1, linetype = "dashed", color = "blue") +
  geom_vline(xintercept = mean_old2, linetype = "dashed", color = "red") +
  labs(x = "Conversion rate",
       y = "Density") +
  annotate("text", x = 0.05, y = 150, label= "Prior: Beta(2,20)") +
  annotate("text", x = 0.19, y = 25, label= "Prior: Beta(1,1000)") +
  scale_color_manual(values = c("red", "blue"))
```
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
We showed the impact with an example. We extracted the first 100 rows of data and selected $Beta(2,20)$ and $Beta(1,1000)$ as the prior distributions of the conversion rate of the old page, and calculated the corresponding posterior distribution. In Figure 2, the red one represented the probability density of the posterior distribution when the prior distribution was $Beta(2,20)$; the blue one represented the probability density of the posterior distribution when the prior distribution was $Beta(1,1000)$.
\end{minipage}

We could see that the probability densities of these two posterior distributions was vastly different; the mean conversion rate of the old page calculated with the prior distribution of $Beta(2,20)$ was about 0.017, while the mean conversion rate of the old page calculated with the prior distribution of $Beta(1,1000)$ was about 0.145. Therefore, one must choose the parameters of the prior distribution carefully in case the sample size is small.

From this point we proceeded with the Bayesian AB test on the given data. We noticed that there were some repetitive data such as columns details were the same except the "timestamp" variable and some data error like "new-page" was in the "control" group, "old-page" was in the "treatment" group.

```{r}
head(new_distinct%>%select(timestamp,group,landing_page,converted),1)%>%kable(booktabs = TRUE,caption = "Data Variable Definition")%>%
  kable_styling(font_size=8, latex_options=c("striped","scale_down","hold_position"))%>%
  row_spec(0,bold=TRUE)
```

\begin{minipage}[t]{0.55\textwidth}
```{r, fig.height=2, fig.width=5, fig.align='center', fig.cap = "Probability of Alternative Hypothesis"}
old_distinct <- old %>% distinct(old$user_id, .keep_all = TRUE)
new_distinct <- new %>% distinct(new$user_id, .keep_all = TRUE)
visitor_to_old <- nrow(old_distinct)
visitor_to_new <- nrow(new_distinct)
conversion_old <- sum(old_distinct$converted)
conversion_new <- sum(new_distinct$converted)

#using non-informative priors, subset size = 100
my_instance <- BayesianSample$new(old_distinct,new_distinct,rows_per_subset = 1000,
                                  alpha_prior_old=1,
                                  alpha_prior_new=1,
                                  beta_prior_old=1,
                                  beta_prior_new=1)
frame0<-my_instance$calculateMean()%>%mutate(frame="Non-informative Prior")

#using informative priors, subset size = 100
my_instance <- BayesianSample$new(old_distinct,new_distinct,rows_per_subset = 1000,
                                  alpha_prior_old=2,
                                  alpha_prior_new=2,
                                  beta_prior_old=20,
                                  beta_prior_new=20)
frame1<-my_instance$calculateMean()%>%mutate(frame="Informative Prior")

p<-rbind(frame0, frame1)%>%select(size, value, frame)%>%
  ggplot(aes(x = size, y = value)) +
  geom_point(shape = 16, color = "blue") +
  facet_wrap(scales = "free", facets = ~frame)+
  labs(x = "Sample Size", y = "P")
print(p)
```
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
For the choice of the parameters of the prior distribution, we had two cases: one was to use a non-informative prior, i.e., to assume that $Beta(1, 1)$ was the prior distribution of the data of the old and new web pages; the other was to use an informative prior, i.e., to assume that $Beta(2, 20)$ was the prior distribution of the data of the old and new web pages. We took sample subset of size 1000 iteratively and used existing posteriors as the new prior distribution for the next subset.
\end{minipage}

The calculated the probability of the alternative hypothesis shown in Figure 3 revealed that in this data set the effect of whether the prior was informative or not was negligent, because of our big sample size. Secondly, we plotted the convergence of the posterior mean as the sample size increases. 

\begin{minipage}[t]{0.55\textwidth}
```{r, fig.width=5, fig.height=2, fig.align='center', fig.cap = "Posterior Mean Comparison"}
#Mean_old & Mean_new Plot
result_mean <- frame1[, -which(names(frame1) == "value")]

#reorganize into long format
result_long1 <- gather(result_mean, key = "Variable", value = "Mean", posterior_old_mean:posterior_new_mean)

ggplot(result_long1, aes(x = size, y = Mean, color = Variable)) +
  geom_line() +
  labs(x = "Sample Size",
       y = "Posterior Mean") +
  scale_color_manual(values = c("posterior_old_mean" = "blue", "posterior_new_mean" = "red"))
```
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
From Figure 4 on the left we noticed that when the sample size was in the range of around 200-12500, the value of the posterior mean for the new webpage was larger than that of the old page, i.e., new web page had higher conversion rates and the difference between the two conversion rates was quite large. However, when the sample size of the subset was larger than 12500, the blue line had been always higher than the red line, but the difference between the two was relatively small.
\end{minipage}

From this, we could infer that overall, the conversion rate of the new web pages was lower than that of the old web page. This result was the opposite of what the web designer would have expected. Figure 5 below was the 95% quantile-based credible intervals for each sample size of the subset of posterior means for the old and new webpages.
```{r, fig.height=3, fig.align='center', fig.cap = "Posterior Mean with Informative Prior"}
# Plot Credible Interval
my_instance$PlotCI()%>%print()
```

As the sample size of the subset increased, the range of the credible intervals became gradually narrower, indicating that: the larger the sample size of the data, the smaller the error caused by sampling, the closer the posterior means were to the true value, and the results were more reliable.

In order to compare the conversion rates of the old and new pages more intuitively, we drew the obtained conversion rate posterior distribution of the old and new pages in an individual graph.

\begin{minipage}[t]{0.55\textwidth}
```{r, fig.width=4, fig.height=3, fig.align='center', fig.cap = "Posterior Mean Comparison with Informative Prior"}
# plot density 
fig_2 <- ggplot() +
  geom_density(aes(x = my_instance$posteriors$posterior_old[[145]]), fill = "blue", alpha = 0.5) +
  geom_density(aes(x = my_instance$posteriors$posterior_new[[145]]), fill = "red", alpha = 0.5) +
  geom_vline(xintercept = last(frame1$posterior_old_mean), linetype = "dashed", color = "blue") +
  geom_vline(xintercept = last(frame1$posterior_new_mean), linetype = "dashed", color = "red") +
  labs(margin=list(l=0, r=0, b=0, t=0),
       x = "Conversion rate",
       y = "Density") +
  annotate("text", x = 0.116, y = 300, label= "New Page") +
  annotate("text", x = 0.1225, y = 300, label= "Old Page") +
  annotate("text", x = 0.1175, y = 50, label = paste("mean of new\n", round(last(frame1$posterior_new_mean), 6)), vjust = 0.5, size = 3) +
  annotate("text", x = 0.1215, y = 50, label = paste("mean of old\n", round(last(frame1$posterior_old_mean), 6)), vjust = 0.5, size = 3) +
  annotate("segment", x = last(frame1$posterior_old_mean), xend = last(frame1$posterior_new_mean), y = 510, yend = 510, colour = "black", linewidth = 1, arrow = arrow()) +
  annotate("text", x = (last(frame1$posterior_old_mean) + last(frame1$posterior_new_mean))/2, y = 480, label = paste(round(last(frame1$posterior_old_mean)-last(frame1$posterior_new_mean),6))) +
  scale_color_manual(values = c("red", "blue"), labels = c("new page", "old page"))

print(fig_2)
```
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
Compared to the old page, the probability density of the new page was shifted to the left. This implied that from an overall level, the performance of the new page was likely to be inferior to that of the old page. Furthermore, the expected conversion rate for the new page in Figure 6 was `r last(frame1$posterior_new_mean)`. However, the expected conversion rate for the old page was `r last(frame1$posterior_old_mean)`. The conversion rate expectation for the new page was `r round(last(frame1$posterior_old_mean)-last(frame1$posterior_new_mean),6)` lower than the conversion rate expectation for the old page. In terms of conversion rate performance, the probability that the new page is better than the old page was only about `r last(my_instance$value)`. In conclusion, by trying a prior of $Beta(2,20)$ in a Bayesian AB test, we rejected the alternative hypothesis and accepted the null that the new webpage seemed to be inferior compared to the old webpage.
\end{minipage}

### Bayesian Regression Analysis

In this section we explained the linear regression techniques using the Bayesian framework.

In the ordinary linear regression analysis, one specifies the model as
\begin{equation*}
\vec{y}=\vec{x}\cdot \vec{\beta}+\vec{\epsilon},
~\vec{y}=\begin{bmatrix} y_1\\ \vdots\\ y_n \end{bmatrix},
~\vec{x}=\begin{bmatrix} 1 & x_{11} & \cdots & x_{1k}\\ \vdots & & \vdots\\ 1 & x_{n1} & \cdots & x_{nk} \end{bmatrix}, 
~\vec{\beta}=\begin{bmatrix} \beta_0 \\ \vdots\\ \beta_{k-1} \end{bmatrix}
\end{equation*}
where 
$\vec{y}$ is the response variable, $\vec{x}$ is the design matrix containing the predictor variables, $\vec{\beta}$ is the unknown parameters of the linear model to be estimated, and $\vec{\epsilon}$ is the random error in the model assumed to have Multivariate Normal distribution $N_n(0_n, \sigma^2 I_n)$. 

In the Bayesian framework, we have the same model structure, however with the additional prior assumption for our unknown parameters. In particular, one assumes that $\sigma^2$ has an InverseGamma(a,b) distribution, and $\vec{\beta} | \sigma^2$ has a mixture distribution of Multivariate Normal $N_k(m, \sigma^2 V$). The resulted posterior distribution is the product of the kernel densities of Multivariate Normal $N_k(\mu, \sigma^2 \Sigma$) and InverseGamma($a^*$, $b^*$), where $\mu:=(x^Tx + V^{-1})^{-1}(x^Ty +V^{-1}m)$, $\Sigma:=(x^Tx + V^{-1})^{-1}$, $a^*:=\frac{n}{2}+a$ and $b^*:=\frac{1}{2}(m^TV^{-1}m - \mu^T \Sigma^{-1}\mu +y^Ty +2b)$. Therefore we have conjugate priors in this case, which is not necessarily possible in general. For simplicity and practicality we adopted this approach. In our analysis, we specified the prior of the variance $\sigma^2$ as general as possible. One could use the reciprocal of $\sigma^2$ which gives an uniform prior, however we did not choose this prior for the reason that the uniform prior is improper with range from $-\infty$ to $+\infty$. We believed that the variance is still finite since a patient cannot stay in the hospital for an infinite period, hence the variance of the length of stay should not go to infinite. In Figure 7 below, we showed a few possible density lines of the InverseGamma distribution. We chose a=2 and b=200 to have a less informative prior.

\begin{minipage}[t]{0.55\textwidth}
```{r, fig.height=2, fig.align='center', fig.cap = "Inverse Gamma Distribution"}
x<-seq(1, 300, 1)
dinvgamma<-function(x, alpha, beta){beta^alpha/gamma(alpha)*x^(-alpha-1)*exp(-beta/x)}
data.frame(x,y1=dinvgamma(x,2,100),y2=dinvgamma(x,2,200),y3=dinvgamma(x, 2,300))%>%
  gather("density", "value", y1:y3)%>%
ggplot(aes(x = x, y = value, color = density)) +
  geom_line() +
  labs(x = "X", y = "PDF") +
  scale_color_discrete(name = "parameters", labels=c("alpha=2, beta=100", "alpha=2, beta=200", "alpha=2, beta=300")) +
  theme_minimal()           
```
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
\vspace{0pt}
To select the appropriate model, we did a predictive check. For every posterior sample we got from the posterior distribution, we ran the model to predict the response variable y with the same predictor variables that was used in the model training/fitting procedure. We then compared the resulting estimated response $\hat{y}$ with the true response y in the training data set by calculating the residual sum of squares $RSS:=\sum_{i=1}^n (\hat{y}-y)^2$.
\end{minipage}

We averaged this value across all the posterior samples. In other words, we were predicting the same response data set that was used to fit the model by using the same observed predictor variables. Intuitively, one would expect that this prediction is perfect, resulting an average RSS equal to 0. It is not possible because of the random nature of our linear model, hence   we chose the model that has the smallest average RSS. 

We proceeded with an exploratory analysis of our second data set. The data set belonged to a larger database called MIMIC-III (Medical Information Mart for Intensive Care) comprising information relating to patients admitted to critical care units at a large tertiary care hospital [@MIMIC]. We first created a brief data dictionary for this data set in Table 2.

```{r}
mimic3d<-read.csv("mimic3d.csv", header = TRUE)

DataDict<-data.frame(
  Variables=colnames(mimic3d),
  Type=sapply(mimic3d, function(x) class(x)),
  Example=sapply(mimic3d, function(x) paste(as.character(head(unique(x),3)), collapse = ", ")),
  Number.Unique=sapply(mimic3d, function(x) length(unique(x))),
  PctMissing=sapply(mimic3d, function(x) paste0(round(sum(is.na(x))/length(x), 4)*100,"%" ) ),
  Comment=c("ID number",
            "Gender",
            "Age",
            "Length of stay in days",
            "Type of admission",
            "Location admitted",
            "Diagnosis when admitted",
            "Insurance",
            "Religion",
            "Marital Status",
            "Ethnicity",
            "time ICU discharged, a.k.a 'called out'",
            "Result of diagnosis",
            "Result of procedures", 
            "Procedured when admitted",  
            "Result of CPT events for billing",
            "Result of drug administered",
            "Result of labs",
            "Result of microbiology labs",
            "Result of notes recorded",
            "Result of fluid excreted by patients",
            "Unknown",
            "Result of Procedures events",
            "Result of transfers",
            "Result of events chart data recorded",
            "If Hospital card has expired",
            "Total number of interactions",
            "Group number of LOSdays"
  )
)

DataDict%>%remove_rownames()%>%kable(booktabs = TRUE,caption = "Data Variable Definition")%>%
  kable_styling(font_size=10, latex_options=c("striped","scale_down","hold_position"))%>%
  column_spec(4, width = "8em")%>%
  row_spec(0,bold=TRUE)

```

The main variable (the response) we were mostly interested in was the length of stays (LOSdays) of a patient in the care unit. Our goal was to show the Bayesian regression model applied to the data set and used to predict the response variable from a train data set. We compared this prediction with the actual test data set to make inference. As in the case of the ordinary linear regression, one must choose the appropriate model that describes the best the data set. We therefore began with model variable selection.

We first of all checked all the categorical variables, namely gender, admit_type, admit_location, AdmitDiagnosis, insurance, religion, marital_status, ethnicity, AdmitProcedure, ExpiredHospital and finally LOSgroupNum. As one can see from Table 2 above, marital_status had 17% of missing values, hence we believed that this variable had limited contribution towards the response. Then admit_location, AdmitDiagnosis, religion, ethnicity, AdmitProcedure had too many unique categories (>5 categories), thus we also ignored the significance of these variables in our regression model. In addition, LOSgroupNum is the group category of the response variable LOSdays, thus also omitted. Therefore, we were left with gender, admit_type, insurance, and ExpiredHospital categorical variables to analyze.

We first performed a Pearson's Chi-squared independence test. Assuming two random variables X and Y with I and J disjoint categories respectively. The contingency table for each i in I and j in J is the frequency table of the number of occurrences. Let $p_{ij}$ be the frequency of an outcome belonging to the ith category of X and the jth category of Y, then one has equivalently the complete joint distribution of X and Y. The Independence Test tests the hypothesis $H_0: ~ p_{ij} = p_i \times p_j \quad H_1: ~ p_{ij} \neq p_i \times p_j$ where $p_i$ $p_j$ are the marginal probabilities $\forall (i,j) \in (I, J)$.

```{R}
# Transform categorical data
mimic3d_cat <- na.omit(subset(mimic3d,select = c(LOSdays, gender, admit_type, insurance, ExpiredHospital)))
cate_data <- mimic3d_cat %>% transmute(
  LOSdays_Less2 = factor(ifelse(LOSdays < 2, 1, 0)), 
  gender = factor(gender, levels = c("F", "M")), 
  admit_type = factor(admit_type, levels = c("ELECTIVE","EMERGENCY","NEWBORN","URGENT")),
  insurance = factor(insurance, levels = c("Government","Medicaid","Medicare","Private","Self Pay")),
  ExpiredHospital=factor(ExpiredHospital))

# Chi_square_test on category variables
CategoryTest <- CategoryDependence$new(cate_data, 
                                       predictor_x=c("gender","admit_type","insurance","ExpiredHospital"),
                                       response_y="LOSdays_Less2")
CategoryTest$p_value
```

From the Chi-squared Test results above, we saw that the predictor “gender” has a p-value greater than 0.05, so we failed to reject the null hypothesis and concluded that there might be independence between gender and response. However, considering the rest of the variables for which we had to reject the independence according to the test, we suspected that perhaps the hypothesis we were trying to test was too strong. We were testing the independence for every ith and jth categories, however in the linear model our goal was not to establish a model that precisely explains the response; rather we were trying to fit the overal correlation between the response and predictors. In practical, one might be able to ignore some of the predictors that is less "explanatory" to the response, even if there exists dependence among them. Therefore, to determine the trade-off of categorical variables, we used the random forest technique from the R package randomForest. This function implements Breiman’s random forest algorithm. We calculated the feature importance to get an insight of which predictors we could further ignore.  

The principle of the algorithm is that the data is divided into predictors and targets (response). In this way we get the data set $Dateset = {\{(X_i,Y_i)\}_{i=1}^{n}}$, where $X_i$ is the $i^{th}$ predictor, $Y_i$ is the $i^{th}$ target. Next, the bootstrap sampling with replacement is done on the data set to form multiple subsets, and then decision trees are constructed individually for each of these subsets. Based on the sampling method, suppose the probability of getting sampled is p, then the probability of NOT getting sampled in n trials is $(1-p)^n$. Taking n to the infinity we had that about 36% of the samples will never appear in any subset of the decision tree. Such samples are called out-of-bag data. After all trees are constructed, the average error on the out-of-bag data is calculated to obtain the out-of-bag error of the model.

For every $i^{th}$ feature, its correlation with the response is destroyed by random permutations and then the model's out-of-bag error is recalculated. The feature importance can be calculated by comparing the out-of-bag error of the original model and the model after destroying the feature correlation (via calculating and compare the average error before and after the disruption). A larger error increase indicates that the feature contributes more to the model's performance and this feature would be considered as a more important feature. Although this method has flaws when dealing with features of different sizes (random forest prefers features with large sizes), in our case, there is no such drawback because the features have the same size.

```{r}
cate_data_WithoutGender <- cate_data %>% select(-gender)
target <- cate_data_WithoutGender$LOSdays_Less2
features <- cate_data_WithoutGender[, -1]

model <- randomForest(x = features, y = target, ntree = 100)

feature_importance <- importance(model)

print(feature_importance)
```

From the above result, we saw the feature importance of ExpiredHospital was much greater than the other two, i.e., ExpiredHospital contributed more to the response. However, ExpiredHospital is a binary outcome (1 indicated death in the hospital, and 0 indicates survival to hospital discharge [@ExpiredH]) indicating whether the patient died within the period of hospitalization. This feature should be logged after the process ends. Logically, in-hospital death should not be used as a feature to predict a patient's length of stay. Therefore, all three remaining categorical variables got ignored in our regression analysis.

For the numerical variables, intuitively we thought that the results for patient input and output, lab test and note, and the result of doing some procedures were not really relevant to the length of stay. We considered the following variables that were definitely related to the response: NumDiagnosis, which represented the time of diagnosis when the patient was admitted, NumTransfers, which showed the number of transfers of the patient between departments of the hospital, TotalNumInteract, which was the total number of interactions between patient and the caregiver, and finally NumCallouts, the time at discharge. We used these four predictors as the base model. We left with age, NumProcEvents and NumChartEvents variable unsure. We therefore performed sequential model fitting by adding each of these three variables to the base model and evaluated if the posterior predictive checking could eliminate any of the variables.

```{r}
mimic3d_clean<-mimic3d%>%select(c("LOSdays", 
                                  "NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts",
                                  "age","NumProcEvents","NumChartEvents"))%>%na.omit()

train_data<-mimic3d_clean[1:47180,]
test_data<-mimic3d_clean[47181:nrow(mimic3d_clean),]
```

```{r}
#Model selection
predictor_x_0<-c("NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")

predictor_x_1<-c("age","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")
predictor_x_2<-c("NumProcEvents","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")
predictor_x_3<-c("NumChartEvents","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")

predictor_x_4<-c("age","NumProcEvents","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")
predictor_x_5<-c("age","NumChartEvents","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")

predictor_x_6<-c("NumProcEvents","NumChartEvents","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")

predictor_x_7<-c("age","NumProcEvents","NumChartEvents","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts")
model_list<-list(predictor_x_0,predictor_x_1,predictor_x_2,predictor_x_3,predictor_x_4,
                 predictor_x_5,predictor_x_6,predictor_x_7)

candidates<-list()
for(i in model_list){
  RegressionTest<-BayesianRegression$new(train_data, responseV=c("LOSdays"), predictorV=i, prior_sigma=c(2,200), prior_beta=list(m=rep(0, (length(i)+1) ), V=diag(nrow=(length(i)+1) )) )
  rss<-RegressionTest$PredictiveCheck(train_data)
  candidates<-c(candidates, list(data.frame(Model=paste(i, collapse = ", "), RSS=round(rss,0) )))
}
candidates%>%do.call(rbind, .)%>%kable(booktabs = TRUE,caption = "Model Selection")%>%
  kable_styling(font_size=8, latex_options=c("striped","scale_down","hold_position"))
```

```{r}
#Bayesian Regression Analysis
RegressionTest<-BayesianRegression$new(train_data, c("LOSdays"), c("NumProcEvents","NumChartEvents","NumDiagnosis","NumTransfers","TotalNumInteract","NumCallouts"), 
                                       prior_sigma=c(2,200), prior_beta=list(m=rep(0, 7), V=diag(nrow=7)) )
```
From the resulted table Table 3 above, we concluded that the full model had the lowest average residual sum of squares. However the second last model without the age predictor had a fairly close average RSS, but the model was one parameter less. Therefore we adopted the simpler model to make our final prediction of the test data set.

Finally we fitted the regression model to the data set and did a prediction of the test data. We obtained the distributions of all the predictor variables:

```{r, out.height="60%", fig.align='center', fig.cap = "Posterior Distributions"}
RegressionTest$PosteriorPlot()%>%print()
```

```{r}
PredictP<-RegressionTest$PredictionPlot(test_data)

saveWidget(widget = PredictP, file = "./Figures/PredictP.html")
shots<-webshot(url = "./Figures/PredictP.html", file = "./Figures/PredictP.png", delay = 2, zoom = 3, vheight = 200, vwidth = 700)
```

```{r, fig.align='center', fig.cap = "Histogram of the True and Predicted Length of Stay"}
knitr::include_graphics("./Figures/PredictP.png")
```

From the predicted result in Figure 9 above, we saw that the distribution of the predicted length of stay was mainly between 5 and 15. Due to the fact that 95% of the observed test data were below `r quantile(test_data$LOSdays, 0.95)`, for a better display purpose we aggregated all values greater than 30 to the count of 30 because the observed response was heavily right skewed with little number of frequencies. The majority of the observed response and the predicted response lied within 95% interval ([`r quantile(test_data$LOSdays, 0.025)`, `r quantile(test_data$LOSdays, 0.975)`] for observed LOSdays and [`r quantile(RegressionTest$responseV_star, 0.025)`, `r quantile(RegressionTest$responseV_star, 0.975)`] for predicted LOSdays). This revealed that the predicted length of stay had the distribution more clustered around the interval. However the shape of the predicted distribution was leaning towards the right-hand size of the true distribution of the test data. One possible way to explain this was that the test data had 5% observations that were greater than 30, which could be observed from the high-frequency green bar on the right. One can expect the same heavy tail in the train data. The right heavy tail characteristic of the train data was influential enough to push the posterior distribution more to the right. However our model was not designed to handle extreme-value cases. Nevertheless, we believed that the Bayesian model was still informative and good in its prediction, in a way that although extreme values weren't explained, the model successfully shifted the distribution to the right, preventing researchers from underestimating the response variable.

Based on our result, we believed that patients most likely had to miss work for more than 2 days. 

### References

<div id="refs"></div>

### Appendix
```{r, echo=TRUE, include=TRUE}
# Test function validity
source("TestSpace.R")
```

